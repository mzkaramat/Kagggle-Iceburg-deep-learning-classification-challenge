{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IceBergDataset(Dataset):\n",
    "    def __init__(self, dataset, offset, length):\n",
    "        self.dataset = dataset\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(dataset) >= offset + length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(IceBergDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        assert i < self.length, Exception(\"index out of range\")\n",
    "        return self.dataset[i + self.offset]\n",
    "\n",
    "\n",
    "def trainTestSplit(dataset, val_share=0.33):\n",
    "    val_offset = int(len(dataset) * (1 - val_share))\n",
    "    return IceBergDataset(dataset, 0, val_offset), IceBergDataset(dataset,\n",
    "                                                                  val_offset, len(dataset) - val_offset)\n",
    "\n",
    "\n",
    "def tensorConvertData(data):\n",
    "    if (use_cuda):\n",
    "        tensor = (torch.from_numpy(data).type(torch.FloatTensor).cuda())\n",
    "    else:\n",
    "        tensor = (torch.from_numpy(data)).type(torch.FloatTensor)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#1604 enteries\n",
    "train = pd.read_json('./Data/trrain/data/processed/train.json')\n",
    "train = shuffle(train)\n",
    "# angle data\n",
    "angles = np.array(train['inc_angle']);\n",
    "# labels (iceberg = 1, ship = 0)\n",
    "targets = np.array(train['is_iceberg']);\n",
    "band_1 = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2 = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "all_images = np.stack([band_1, band_2], axis=1)  # (1604, 2, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images.shape\n",
    "num_epochs = 5\n",
    "batch_size = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "use_cuda = torch.cuda.is_available();\n",
    "\n",
    "images = tensorConvertData(all_images);\n",
    "targets = targets.reshape((targets.shape[0], 1))\n",
    "targets = tensorConvertData(targets);\n",
    "dataset = TensorDataset(images, targets);\n",
    "dataset_train, dataset_val = trainTestSplit(dataset)\n",
    "\n",
    "dataloader = DataLoader(dataset_train, batch_size=batch_size, num_workers=1)\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(75 * 75, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 75 * 75), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
